{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Ruddit dataset\n",
    "This notebook is used for preprocessing of the [Ruddit](https://github.com/hadarishav/Ruddit) dataset. The dataset is a collection of Reddit comments, which are annotated for offensiveness. Since the original repository for Ruddit dataset only contains the ID of the posts and comments, the [Ruddit dataset shared in Kaggle](https://www.kaggle.com/datasets/rajkumarl/ruddit-jigsaw-dataset) with the texts already extracted from Reddit was used in this notebook. For running this notebook, please first create the `data` directory inside the `src` folder (same directory that contains this notebook file), then download the dataset from Kaggle and place the `ruddit` directory it in the `data/` directory.\n",
    "\n",
    "After running this notebook, the preprocessed csv files for train, test and validation splits will be saved in the `data/ruddit/Preprocessed` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set matplotlib dpi to 300\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUDDIT_PATH = \"data/ruddit/Dataset/ruddit_with_text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RUDDIT_PATH)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete rows with txt == '[deleted]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop url column\n",
    "df.drop(\"url\", axis=1, inplace=True)\n",
    "# Rename txt column to text\n",
    "df = df.rename(columns={'txt': 'text'})\n",
    "# Show number of deleted posts\n",
    "deleted = df[\"text\"] == '[deleted]'\n",
    "print(\"Deleted posts: \", len(df[deleted]))\n",
    "# Drop rows with deleted text\n",
    "df = df[~deleted]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the Offensiveness Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = (df[\"offensiveness_score\"].values + 1.) / 2.\n",
    "df.drop(\"offensiveness_score\", axis=1, inplace=True)\n",
    "\n",
    "print(\"Offensive Score:\")\n",
    "print(f\"Mean: {df['label'].mean()}\")\n",
    "print(f\"Std: {df['label'].std()}\")\n",
    "print(f\"Min: {df['label'].min()}\")\n",
    "print(f\"Max: {df['label'].max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Distribution of the Offensive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the offensive score\n",
    "sns.kdeplot(df['label'], fill=True)\n",
    "plt.title(\"Distribution of Offensive Score\")\n",
    "plt.xlabel(\"Offensive Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_test_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the data distribution of the train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(train_df[\"label\"], label=\"Train Set\")\n",
    "sns.kdeplot(val_df[\"label\"], label=\"Validation Set\")\n",
    "sns.kdeplot(test_df[\"label\"], label=\"Test Set\")\n",
    "plt.title(\"Distribution of Offensive Score\")\n",
    "plt.xlabel(\"Offensive Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pairs(df, n=5):\n",
    "    # Create a list to store the results\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each row in the original dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        text1 = row['text']\n",
    "        label1 = row['label']\n",
    "        \n",
    "        # Randomly select n other rows from the original dataframe, excluding the current row\n",
    "        random_rows = df.drop(index).sample(n=n)\n",
    "        \n",
    "        for _, random_row in random_rows.iterrows():\n",
    "            text2 = random_row['text']\n",
    "            label2 = random_row['label']\n",
    "            \n",
    "            # Determine bin_label based on which text has a higher positivity score\n",
    "            if label1 > label2:\n",
    "                bin_label = 0\n",
    "            else:\n",
    "                bin_label = 1\n",
    "            \n",
    "            # Calculate the difference in labels\n",
    "            dif_label = label1 - label2\n",
    "            \n",
    "            # Append the new row to the list of data\n",
    "            data.append({'text1': text1, 'text2': text2, 'bin_label': bin_label, 'dif_label': dif_label})\n",
    "\n",
    "    # Create a new dataframe from the list of data\n",
    "    new_df = pd.DataFrame(data)\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the train, validation, and test sets to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pair_df, val_pair_df, test_pair_df = create_sentence_pairs(train_df), create_sentence_pairs(val_df), create_sentence_pairs(test_df)\n",
    "\n",
    "\n",
    "# Print the count of bin_label in each dataframe\n",
    "print(\"Train Set:\")\n",
    "print(train_pair_df['bin_label'].value_counts())\n",
    "print(\"Validation Set:\")\n",
    "print(val_pair_df['bin_label'].value_counts())\n",
    "print(\"Test Set:\")\n",
    "print(test_pair_df['bin_label'].value_counts())\n",
    "\n",
    "# Create Preprocessed folder in data/ruddit directory if it does not exist\n",
    "if not os.path.exists(\"data/ruddit/Preprocessed\"):\n",
    "    os.makedirs(\"data/ruddit/Preprocessed\")\n",
    "\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "train_pair_df.to_csv(\"data/ruddit/Preprocessed/train_pair.csv\", index=False)\n",
    "val_pair_df.to_csv(\"data/ruddit/Preprocessed/val_pair.csv\", index=False)\n",
    "test_pair_df.to_csv(\"data/ruddit/Preprocessed/test_pair.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
